{"cells":[{"metadata":{},"cell_type":"markdown","source":"# My First Neural Network\nAfter finishing a few of the great courses provided here on [kaggle/learn](https://www.kaggle.com/learn/overview), I decided to build my first NN model using this dataset. <BR>\nThe mission is to predict bone age using hand X-ray images and their associated age (in months).\n<BR>\n<BR>\nI hope this notebook will provide a guideline for other beginners like me as they do their first steps in building NN models"},{"metadata":{},"cell_type":"markdown","source":"# Project Setup\n### Imports and Hiperparameters\nI decided to gather all the used modules in one cell to provide a better understanding of the project and to maintain order."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.metrics import mean_absolute_error\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hiper parameters\nimg_rows, img_cols = 128, 128 # 384, 384\nimage_size = (img_rows, img_cols)\ntrain_batch_size = 32\nvalid_batch_size = 256\nEPOCHS = 100\nSTEPS_PER_EPOCH = lambda X: (0.8*len(X)) // train_batch_size #0.8 is a set parameter also\nVALIDATION_STEPS = lambda y: (0.2*len(y)) // valid_batch_size #0.2 is a set parameter also","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the Data Files\nThe data provided in this project includes train images and their associated csv file (containing the bonage for each image) and also test images without their associated csv file. <BR>\nThe reason for that is because the way Kaggle competitions work - you need to submit a csv containing your predictions for the test data to get your prediction score.\n<BR>\nDue to the fact that the competition was already closed when I decided to work on this project, I  evaluated my results on my own using the test csv file found [here](https://stanfordmedicine.app.box.com/s/4r1zwio6z6lrzk7zw[](http://)3fro7ql5mnoupcv)."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_file = \"../input/rsna-bone-age/boneage-training-dataset.csv\"\ntest_file = \"../input/bone-age-ground-truth-csv/Bone age ground truth.csv\"\ndata_dir = '../input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset'\ntest_dir = \"../input/rsna-bone-age/boneage-test-dataset/boneage-test-dataset\"\n\nall_data = pd.read_csv(data_file, dtype=str)\n\n# Loading test data in the same format as all data\nall_test = pd.read_csv(test_file, dtype=str,\n                       names=['id', 'male', 'boneage'],\n                       header=0)\n\n# Creating a path column to correclty load the images using ImageDataGenerator\nall_data['path'] = [\"{}.png\".format(i) for i in all_data['id']]\nall_test['path'] = [\"{}.png\".format(i) for i in all_test['id']]\n\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As customary, splitting the data into train and validation sets\ntrain, valid = train_test_split(all_data, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Image Data Generators\n\nKeras’ ImageDataGenerator class is a class used to augment data and create batches from it. <BR>\nThe class’ constructor accepts many augmentation arguments you can read about [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_dataframe). <BR>\nI used those augmentation arguments to create the following generators:\n* data_generator - used for training, data is augmented so the model will learn various versions of possible bone images.\n* valid_generator - used for validation, data is **not** augmented so validation set evaluation will be similar to the test set evaluation.\n* test_generator - used for testing, never should be augmented.\n<BR>\n<BR>\nAll of the mentioned generators are processed with the same preprocess function (and thus are in the same format)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_generator = ImageDataGenerator(preprocessing_function=preprocess_input, \n                                    rotation_range=20, \n                                    width_shift_range=0.2, \n                                    height_shift_range=0.2, \n                                    zoom_range=0.2, \n                                    horizontal_flip=True,\n                                    validation_split=0.2,\n                                    fill_mode='nearest')\n\nvalid_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n\ntest_generator = ImageDataGenerator(preprocessing_function=preprocess_input)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using those generators, we create objects from the 'flow_from_dataframe' method. <BR>\nThis method creates an iterator that iterates over the data in batches (determined by 'batch_size' argument) and returns both the images and their associated boneage. <BR>\nI recommend reading more about this method and the ImageDataGenerator class entirely."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = data_generator.flow_from_dataframe(\n    dataframe=train,\n    directory=data_dir,\n    x_col='path',\n    y_col='boneage',\n    target_size=image_size,\n    color_mode='rgb',\n    class_mode='sparse',\n    batch_size=train_batch_size,\n    shuffle=True,\n    seed=42,\n    interpolation='nearest',\n    validate_filenames=True)\n\nvalid_gen = valid_generator.flow_from_dataframe(\n    dataframe=valid,\n    directory=data_dir,\n    x_col='path',\n    y_col='boneage',\n    target_size=image_size,\n    color_mode='rgb',\n    class_mode='sparse',\n    batch_size=valid_batch_size,\n    shuffle=True,\n    seed=42,\n    interpolation='nearest',\n    validate_filenames=True)\n\ntest_gen = test_generator.flow_from_dataframe(\n    dataframe=all_test,\n    directory=test_dir,\n    x_col='path',\n    y_col='boneage',\n    target_size=image_size,\n    color_mode='rgb',\n    classes=None,\n    class_mode='sparse',\n    batch_size=200,\n    shuffle=False,\n    seed=42,\n    interpolation='nearest',\n    validate_filenames=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Viewing Some Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_x, t_y = next(train_gen)\nfig, m_axs = plt.subplots(2, 4, figsize=(16, 8))\nfor (c_x, c_y, c_ax, ind) in zip(t_x, t_y, m_axs.flatten(), range(8)):\n    c_ax.imshow(c_x[:,:,0], cmap='bone', vmin=-127, vmax=127)\n    c_ax.set_title('Age: %fY\\n' % (t_y[ind]/12.0))\n    c_ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Creation"},{"metadata":{},"cell_type":"markdown","source":"As I learned from the DL course and after a lot of experiments with hiperparameters and layer construction, this is the best model, regarding test results, in the computational limit provided by kaggle, I could achieve:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n# Input Layer\nmodel.add(Conv2D(16, kernel_size=(7,7), strides=(2,2), activation='relu', input_shape=(img_rows, img_cols, 3)))\nmodel.add(Dropout(0.25))\n\n# Convulotional Layers\nmodel.add(Conv2D(32, kernel_size=(5,5), strides=(2,2), activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\nmodel.add(Dropout(0.25))\n\n# Flattening\nmodel.add(Flatten())\n\n# Dense Layer\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', \n              loss='mse',\n              metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#early stopping\nearly_stopping = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience= 10,\n                              verbose=0, \n                              mode='auto')\n\n#model checkpoint\nmc = ModelCheckpoint('best_model.h5', \n                     monitor='val_loss', \n                     mode='min', \n                     save_best_only=True)\n\n#reduce lr on plateau\nred_lr_plat = ReduceLROnPlateau(monitor='val_loss', \n                                factor=0.1, \n                                patience=5, \n                                verbose=0, \n                                mode='min', \n                                min_delta=0.0001, \n                                cooldown=0, \n                                min_lr=0)\n\nCALLBACKS = [early_stopping, mc, red_lr_plat]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the model\nhistory = model.fit_generator(train_gen,\n    steps_per_epoch = STEPS_PER_EPOCH(all_data),\n    validation_data = valid_gen,\n    validation_steps = VALIDATION_STEPS(all_data),\n    epochs = EPOCHS,\n    callbacks = CALLBACKS)\n\nhistory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results & Model Evalutaion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation loss values\nfig, ax = plt.subplots(figsize = (10,6))\nax.plot(history.history['mae'])\nax.plot(history.history['val_mae'])\nplt.title('Model error (mae)')\nplt.ylabel('error')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nax.grid(color='black')\nplt.show()\nfig.savefig('train_validation_loss_graph', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the best model (saved with the checkpoint callback)\nmodel.load_weights('best_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test data evaluation regarding the loss and metric functions defined in the model (mae in our case).\nmodel.evaluate(test_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing some test images with their true and predicted images\n\ntest_X, test_Y = next(valid_gen)\npred = model.predict(test_X, verbose=True)\ntest_months = test_Y\n\nord_ind = np.argsort(test_Y)\nord_ind = ord_ind[np.linspace(0, len(ord_ind)-1, 8).astype(int)] # take 8 evenly spaced ones\nfig, axs = plt.subplots(2, 4, figsize=(16, 8))\nfor (ind, ax) in zip(ord_ind, axs.flatten()):\n    ax.imshow(test_X[ind, :,:,0], cmap='bone')\n    ax.set_title('Age: %fY\\nPredicted Age: %fY' % (test_months[ind]/12.0, \n                                                           pred[ind]/12.0))\n    ax.axis('off')\nfig.savefig('trained_image_predictions.png', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Future Work\n\nThis project can be massively improved with\n\n* Higher resolution images (I used 128x128 due to computational limitations).\n* Better data sampling for training and validation (the classes are not balanced among train and valid sets).\n* Data normalization - both rescaling the pixels and normalizing the 'boneage' data.\n* Using pretrained models - like the other notebooks working on this dataset did.\n* Improving layers construction - adding maxpooling methods and such."},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\nAfter spending many hours deciding which model design benfits the best results, I concluded that with the computation and time limit in Kaggle, my model is'nt going to reach very good results. <BR>\nHowever, building this model was a challenging experience that taught me a lot about DL and NN models.\n<BR> I definitely recommend this dataset and mission for beginners trying out their first NN."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}